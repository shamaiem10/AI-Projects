{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e4bcdd",
   "metadata": {},
   "source": [
    "## What is RAG ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27668802",
   "metadata": {},
   "source": [
    "RAG is a technique where a language model first looks up (retrieves) useful information from a database or documents, and then uses that information to give a better answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee7c36",
   "metadata": {},
   "source": [
    "## Why and when we prefer RAG over finetuning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632852af",
   "metadata": {},
   "source": [
    "We **prefer RAG over finetuning** when: We want the model to give **up-to-date or specific answers** from **our own data** without changing the model itself.\n",
    "\n",
    "---\n",
    "\n",
    "**Why prefer RAG?**\n",
    "\n",
    "* **Cheaper & faster** – No need to train the model again.\n",
    "* **Easier to update** – Just change the documents, not the model.\n",
    "* **Better for private or large data** – You keep data separate and safe.\n",
    "\n",
    "---\n",
    "\n",
    "**When to use RAG?**\n",
    "\n",
    "* When your data **changes often** (like news, product lists).\n",
    "* When you want the model to **answer from your documents**.\n",
    "* When **training a model is too costly or slow**.\n",
    "\n",
    "---\n",
    "\n",
    "Think of RAG like **giving the model a library to read** instead of teaching it everything from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac356886",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0436fd6",
   "metadata": {},
   "source": [
    "* **`langchain`** – Core framework to build LLM-powered applications.\n",
    "* **`langchain-community`** – Extra integrations like tools, APIs, and vector stores.\n",
    "* **`langchain-pinecone`** – Connects LangChain with Pinecone for vector storage and retrieval.\n",
    "* **`langchain_groq`** – Enables LangChain to use Groq's ultra-fast language models.\n",
    "* **`datasets`** – Provides ready-to-use NLP/ML datasets from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816e50ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.23\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community==0.3.21\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-pinecone==0.2.5\n",
      "  Downloading langchain_pinecone-0.2.5-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting datasets==3.5.0\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain==0.3.23)\n",
      "  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain==0.3.23)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain==0.3.23)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.23) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.23) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.23) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain==0.3.23) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community==0.3.21) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community==0.3.21) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.21)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.21)\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community==0.3.21)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community==0.3.21) (2.1.3)\n",
      "Collecting pinecone<7.0.0,>=6.0.0 (from pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5)\n",
      "  Downloading pinecone-6.0.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community==0.3.21)\n",
      "  Downloading aiohttp-3.10.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone==0.2.5)\n",
      "  Downloading langchain_tests-0.3.22-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets==3.5.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets==3.5.0) (21.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.5.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets==3.5.0) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets==3.5.0) (4.67.1)\n",
      "Collecting xxhash (from datasets==3.5.0)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.5.0)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.5.0)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets==3.5.0) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets==3.5.0) (24.2)\n",
      "Requirement already satisfied: groq<1,>=0.30.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_groq) (0.31.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq<1,>=0.30.0->langchain_groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq<1,>=0.30.0->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from groq<1,>=0.30.0->langchain_groq) (4.14.1)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<1.0.0,>=0.3.51->langchain==0.3.23)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pytest<9.0.0,>=7.0.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest-8.4.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting pytest-asyncio<2.0.0,>=0.20.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_asyncio-1.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting syrupy<5.0.0,>=4.0.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pytest-socket<1.0.0,>=0.7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_codspeed-4.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting vcrpy<8.0.0,>=7.0.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain==0.3.23)\n",
      "  Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.0/43.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain==0.3.23)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain==0.3.23)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2025.8.3)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain==0.3.23) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain==0.3.23) (3.10)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.23) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.66.3->datasets==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets==3.5.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets==3.5.0) (2024.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (3.0.0)\n",
      "Collecting iniconfig>=1 (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.18.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.5.3->pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (1.16.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting wrapt (from vcrpy<8.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (0.2.1)\n",
      "Collecting py-cpuinfo (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: cffi>=1.17.1 in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.0.0)\n",
      "Collecting rich>=13.8.1 (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shama\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.1/1.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.0 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.5/1.0 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.8/1.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.5 MB 8.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.5 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.2/2.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.5 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.0/2.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading langchain_pinecone-0.2.5-py3-none-any.whl (16 kB)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.2 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 399.4/491.2 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 491.2/491.2 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n",
      "Downloading aiohttp-3.10.11-cp311-cp311-win_amd64.whl (382 kB)\n",
      "   ---------------------------------------- 0.0/382.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  378.9/382.6 kB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 382.6/382.6 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "   ---------------------------------------- 0.0/183.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 183.9/183.9 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
      "   ---------------------------------------- 0.0/449.8 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 307.2/449.8 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 449.8/449.8 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading langchain_tests-0.3.22-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB ? eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "   ---------------------------------------- 0.0/363.0 kB ? eta -:--:--\n",
      "   --------------------------------------  358.4/363.0 kB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 363.0/363.0 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading pinecone-6.0.2-py3-none-any.whl (421 kB)\n",
      "   ---------------------------------------- 0.0/421.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  419.8/421.9 kB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 421.9/421.9 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "   ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.6/48.6 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 131.4/131.4 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading pytest-8.4.2-py3-none-any.whl (365 kB)\n",
      "   ---------------------------------------- 0.0/365.8 kB ? eta -:--:--\n",
      "   --------------------------------------  358.4/365.8 kB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 365.8/365.8 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading pytest_asyncio-1.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.5/54.5 kB ? eta 0:00:00\n",
      "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.2/52.2 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.3/42.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "   ---------------------------------------- 0.0/495.4 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 317.4/495.4 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 495.4/495.4 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.3/44.3 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading pytest_codspeed-4.1.1-py3-none-any.whl (113 kB)\n",
      "   ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 113.6/113.6 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "   ---------------------------------------- 0.0/243.4 kB ? eta -:--:--\n",
      "   -------------------------------------- - 235.5/243.4 kB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 243.4/243.4 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.3/87.3 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: py-cpuinfo, zstandard, xxhash, wrapt, pluggy, pinecone-plugin-interface, orjson, mypy-extensions, mdurl, marshmallow, jsonpatch, iniconfig, httpx-sse, fsspec, dill, vcrpy, typing-inspect, requests-toolbelt, pytest, pinecone, multiprocess, markdown-it-py, aiohttp, syrupy, rich, pytest-socket, pytest-recording, pytest-benchmark, pytest-asyncio, pydantic-settings, langsmith, dataclasses-json, pytest-codspeed, langchain-core, datasets, langchain-text-splitters, langchain-tests, langchain_groq, langchain-pinecone, langchain, langchain-community\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.11\n",
      "    Uninstalling aiohttp-3.11.11:\n",
      "      Successfully uninstalled aiohttp-3.11.11\n",
      "Successfully installed aiohttp-3.10.11 dataclasses-json-0.6.7 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 httpx-sse-0.4.3 iniconfig-2.1.0 jsonpatch-1.33 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.79 langchain-pinecone-0.2.5 langchain-tests-0.3.22 langchain-text-splitters-0.3.11 langchain_groq-0.3.8 langsmith-0.3.45 markdown-it-py-4.0.0 marshmallow-3.26.1 mdurl-0.1.2 multiprocess-0.70.16 mypy-extensions-1.1.0 orjson-3.11.3 pinecone-6.0.2 pinecone-plugin-interface-0.0.7 pluggy-1.6.0 py-cpuinfo-9.0.0 pydantic-settings-2.11.0 pytest-8.4.2 pytest-asyncio-1.2.0 pytest-benchmark-5.1.0 pytest-codspeed-4.1.1 pytest-recording-0.13.4 pytest-socket-0.7.0 requests-toolbelt-1.0.0 rich-14.2.0 syrupy-4.9.1 typing-inspect-0.9.0 vcrpy-7.0.0 wrapt-1.17.3 xxhash-3.6.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pinecone 6.0.2 does not provide the extra 'async'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\shama\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.3.23 langchain-community==0.3.21 langchain-pinecone==0.2.5 langchain_groq datasets==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00464f",
   "metadata": {},
   "source": [
    "## Load API Keys from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22aa0b2",
   "metadata": {},
   "source": [
    "### What is env file?\n",
    "\n",
    "* A .env file is a simple text file that stores environment variables (like API keys and secrets) in key=value format.\n",
    "* Example content of a .env file:\n",
    "* PINECONE_API_KEY=your_pinecone_api_key_here\n",
    "* GROQ_API_KEY=your_groq_api_key_here\n",
    "\n",
    "* It helps keep sensitive information out of your code and makes it easier to manage secrets securely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a4c86",
   "metadata": {},
   "source": [
    "**Imports tools** to:\n",
    "\n",
    "* Use environment variables (`os`)\n",
    "* Load values from a `.env` file (`load_dotenv`)\n",
    "* **os** is a Python built-in module that lets your code interact with the operating system (like Windows, macOS, Linux).\n",
    "---\n",
    "* **Loads the `.env` file** so Python can use the secret keys stored in it (like API keys).\n",
    "* **Gets the values** of `PINECONE_API_KEY` and `GROQ_API_KEY` from the `.env` file.\n",
    "* **In Short:** This code **reads your secret keys from a `.env` file** so you don’t have to write them directly in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e6bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone API Key Loaded Successfully\n",
      "✅ Groq API Key Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the keys\n",
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded properly\n",
    "if pinecone_key:\n",
    "    print(\"✅ Pinecone API Key Loaded Successfully\")\n",
    "else:\n",
    "    print(\"❌ Pinecone API Key NOT Loaded\")\n",
    "\n",
    "if groq_key:\n",
    "    print(\"✅ Groq API Key Loaded Successfully\")\n",
    "else:\n",
    "    print(\"❌ Groq API Key NOT Loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1c075",
   "metadata": {},
   "source": [
    "## What is `langchain_groq`?\n",
    "\n",
    "`langchain_groq` is a **LangChain integration** that lets you **connect to Groq’s LLMs** (like LLaMA3) easily.\n",
    "\n",
    "Think of it as a **bridge between LangChain and Groq’s fast language models**.\n",
    "\n",
    "---\n",
    "\n",
    "## What is `ChatGroq`?\n",
    "\n",
    "`ChatGroq` is a **class (tool)** inside `langchain_groq`.\n",
    "\n",
    "It lets you:\n",
    "\n",
    "* **Send prompts** to Groq-hosted models\n",
    "* **Receive responses** from those models\n",
    "* Use these models in your **LangChain app**, like chatbots, RAG, agents, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we use this?**\n",
    "\n",
    "Instead of manually setting up HTTP requests to Groq’s API, `ChatGroq` makes it **super easy**:\n",
    "\n",
    "* Lets you talk to a specific Groq model (`llama3-8b-8192`)\n",
    "* Works smoothly with LangChain tools (retrievers, chains, memory, etc.)\n",
    "* Connects securely with your `groq_api_key`\n",
    "\n",
    "---\n",
    "\n",
    "#### In Simple Words\n",
    "\n",
    "* `langchain_groq` lets LangChain talk to Groq.\n",
    "* `ChatGroq` is the tool that helps you **chat with Groq’s AI model** using your API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "990bfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    groq_api_key=groq_key,\n",
    "    model_name=\"llama-3.1-8b-instant\"  # Correct model name used by Groq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09520cec",
   "metadata": {},
   "source": [
    "Groq uses the **same chat structure as OpenAI** because it runs **OpenAI-compatible models** like `llama3`, `mixtral`, etc.\n",
    "So just like OpenAI, chats with Groq **typically look like this in plain text**:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "User: Hi, how are you?\n",
    "Assistant: I'm doing well! How can I assist you today?\n",
    "User: What is quantum computing?\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "---\n",
    "\n",
    "**In Code (OpenAI/Groq-compatible format):**\n",
    "\n",
    "When using the API (like with `ChatGroq` or `ChatOpenAI` in LangChain), you use this structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**In LangChain (message objects):**\n",
    "\n",
    "LangChain wraps those into **message classes**, like:\n",
    "\n",
    "```\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What is quantum computing?\")\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21b712",
   "metadata": {},
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "Then you pass them to the model:\n",
    "\n",
    "```\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8254e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aab48e",
   "metadata": {},
   "source": [
    "We generate the next response from the AI by passing these messages to the `ChatGroq` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ddade",
   "metadata": {},
   "source": [
    "Like saying to the AI:\n",
    "\n",
    "“Here’s what has been said so far — now tell me what the AI should say next.”\n",
    "\n",
    "LangChain then handles formatting and sending this to the LLM backend, and res stores the AI’s next reply.\n",
    "\n",
    "**In Short:**\n",
    "* You define a conversation (via messages).\n",
    "* Call the LLM using chat(messages).\n",
    "* Get a response back — stored in res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "109d2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d420165f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mres\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f108837",
   "metadata": {},
   "source": [
    "To see the models reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "748f8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity, two theories that are known to be incompatible within the framework of classical physics. It's a complex and mind-bending concept, but I'll try to break it down for you in simple terms.\n",
      "\n",
      "**What is String Theory?**\n",
      "\n",
      "String theory posits that the fundamental building blocks of the universe are not particles, but tiny, vibrating strings. These strings are too small to be seen, but they vibrate at different frequencies, giving rise to the various particles we observe in the universe, such as electrons, photons, and quarks.\n",
      "\n",
      "**The Three Main Assumptions of String Theory:**\n",
      "\n",
      "1. **Strings are the fundamental objects**: Instead of particles being the fundamental units of the universe, strings are the basic building blocks.\n",
      "2. **Strings vibrate at different frequencies**: The different vibrational modes of the strings give rise to the various particles we observe in the universe.\n",
      "3. **Extra dimensions exist**: String theory requires the existence of additional dimensions beyond the three spatial dimensions (length, width, and height) and one time dimension that we experience in everyday life.\n",
      "\n",
      "**The Problem with String Theory:**\n",
      "\n",
      "String theory is still a highly speculative and incomplete theory, and it has yet to be experimentally confirmed. One of the main challenges is that it requires the existence of additional dimensions, which are not directly observable. Additionally, string theory predicts an infinite number of possible universes, making it difficult to make testable predictions.\n",
      "\n",
      "**Types of String Theories:**\n",
      "\n",
      "There are five consistent superstring theories, each requiring the existence of ten dimensions:\n",
      "\n",
      "1. **Type I string theory**: Incorporates both open and closed strings.\n",
      "2. **Type IIA string theory**: Requires the existence of supersymmetry (a theoretical concept that proposes the existence of particles with equal mass but opposite spin).\n",
      "3. **Type IIB string theory**: Similar to Type IIA, but with a different set of supersymmetry constraints.\n",
      "4. **Heterotic SO(32) string theory**: A string theory that combines the features of Type I and heterotic string theories.\n",
      "5. **Heterotic E8×E8 string theory**: Another type of heterotic string theory, which requires the existence of two sets of supersymmetry constraints.\n",
      "\n",
      "**Current Research and Developments:**\n",
      "\n",
      "String theory is an active area of research, with many scientists working to develop new tools and techniques to test its predictions. Some of the current research areas include:\n",
      "\n",
      "1. **String phenomenology**: The study of how string theory can be used to make predictions about particle physics and cosmology.\n",
      "2. **String cosmology**: The study of how string theory can be used to understand the early universe and the evolution of the cosmos.\n",
      "3. **Black hole physics**: The study of how string theory can be used to understand the behavior of black holes.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "String theory is a complex and highly speculative theory that attempts to reconcile quantum mechanics and general relativity. While it has not yet been experimentally confirmed, it remains an active area of research, with many scientists working to develop new tools and techniques to test its predictions.\n",
      "\n",
      "If you have any specific questions or would like further clarification on any of these concepts, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d2d69",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c75ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory can produce a unified theory for several reasons:\n",
      "\n",
      "**1. Unification of Forces:**\n",
      "\n",
      "String theory attempts to unify the fundamental forces of nature, including:\n",
      "\n",
      "* **Gravity**: Described by general relativity\n",
      "* **Electromagnetism**: Described by quantum electrodynamics (QED)\n",
      "* **Weak Nuclear Force**: Described by the electroweak theory\n",
      "* **Strong Nuclear Force**: Described by quantum chromodynamics (QCD)\n",
      "\n",
      "String theory provides a framework for unifying these forces, potentially explaining why they are distinct at low energies but become unified at high energies.\n",
      "\n",
      "**2. Grand Unified Theories (GUTs):**\n",
      "\n",
      "String theory can be thought of as a GUT, where the fundamental forces are unified at a high-energy scale. This is achieved through the existence of extra dimensions, which allow for the unification of forces that would otherwise be separate.\n",
      "\n",
      "**3. Supersymmetry:**\n",
      "\n",
      "String theory requires the existence of supersymmetry (SUSY), which proposes that every particle has a supersymmetric partner particle with equal mass but opposite spin. SUSY is thought to be broken at low energies, but it could play a crucial role in unifying forces at high energies.\n",
      "\n",
      "**4. Kaluza-Klein Theories:**\n",
      "\n",
      "String theory can be thought of as a Kaluza-Klein (KK) theory, which proposes that the fundamental forces are unified in a higher-dimensional space. The extra dimensions are \"compactified\" or \"curled up\" in such a way that they are not directly observable, but they play a crucial role in unifying forces.\n",
      "\n",
      "**5. Mathematical Consistency:**\n",
      "\n",
      "String theory has a high degree of mathematical consistency, which is a hallmark of a well-defined and predictive theory. The theory is based on a consistent set of mathematical equations, which provide a framework for understanding the behavior of particles and forces.\n",
      "\n",
      "**6. Black Hole Physics:**\n",
      "\n",
      "String theory has been successful in explaining certain features of black hole physics, such as the Bekenstein-Hawking formula for black hole entropy. This has led some physicists to believe that string theory may be the correct framework for understanding the behavior of black holes.\n",
      "\n",
      "**7. Particle Physics:**\n",
      "\n",
      "String theory has been successful in predicting certain features of particle physics, such as the existence of Kaluza-Klein modes and the behavior of particles at high energies. These predictions have been confirmed by experiments, providing evidence for the validity of string theory.\n",
      "\n",
      "**8. Cosmology:**\n",
      "\n",
      "String theory has been successful in explaining certain features of cosmology, such as the behavior of the universe at very early times and the formation of structure in the universe. These predictions have been confirmed by observations, providing evidence for the validity of string theory.\n",
      "\n",
      "While string theory is still a highly speculative theory, these reasons have led many physicists to believe that it may be the correct framework for a unified theory of the fundamental forces of nature.\n",
      "\n",
      "Keep in mind that string theory is still an active area of research, and many questions remain unanswered. However, the potential for a unified theory of the fundamental forces of nature makes string theory an exciting and promising area of study.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c7774",
   "metadata": {},
   "source": [
    "## Dealing with Hallucinations\n",
    "\n",
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about Deepseek R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d2b6ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory can produce a unified theory for several reasons:\n",
      "\n",
      "**1. Unification of Forces:**\n",
      "\n",
      "String theory attempts to unify the fundamental forces of nature, including:\n",
      "\n",
      "* **Gravity**: Described by general relativity\n",
      "* **Electromagnetism**: Described by quantum electrodynamics (QED)\n",
      "* **Weak Nuclear Force**: Described by the electroweak theory\n",
      "* **Strong Nuclear Force**: Described by quantum chromodynamics (QCD)\n",
      "\n",
      "String theory provides a framework for unifying these forces, potentially explaining why they are distinct at low energies but become unified at high energies.\n",
      "\n",
      "**2. Grand Unified Theories (GUTs):**\n",
      "\n",
      "String theory can be thought of as a GUT, where the fundamental forces are unified at a high-energy scale. This is achieved through the existence of extra dimensions, which allow for the unification of forces that would otherwise be separate.\n",
      "\n",
      "**3. Supersymmetry:**\n",
      "\n",
      "String theory requires the existence of supersymmetry (SUSY), which proposes that every particle has a supersymmetric partner particle with equal mass but opposite spin. SUSY is thought to be broken at low energies, but it could play a crucial role in unifying forces at high energies.\n",
      "\n",
      "**4. Kaluza-Klein Theories:**\n",
      "\n",
      "String theory can be thought of as a Kaluza-Klein (KK) theory, which proposes that the fundamental forces are unified in a higher-dimensional space. The extra dimensions are \"compactified\" or \"curled up\" in such a way that they are not directly observable, but they play a crucial role in unifying forces.\n",
      "\n",
      "**5. Mathematical Consistency:**\n",
      "\n",
      "String theory has a high degree of mathematical consistency, which is a hallmark of a well-defined and predictive theory. The theory is based on a consistent set of mathematical equations, which provide a framework for understanding the behavior of particles and forces.\n",
      "\n",
      "**6. Black Hole Physics:**\n",
      "\n",
      "String theory has been successful in explaining certain features of black hole physics, such as the Bekenstein-Hawking formula for black hole entropy. This has led some physicists to believe that string theory may be the correct framework for understanding the behavior of black holes.\n",
      "\n",
      "**7. Particle Physics:**\n",
      "\n",
      "String theory has been successful in predicting certain features of particle physics, such as the existence of Kaluza-Klein modes and the behavior of particles at high energies. These predictions have been confirmed by experiments, providing evidence for the validity of string theory.\n",
      "\n",
      "**8. Cosmology:**\n",
      "\n",
      "String theory has been successful in explaining certain features of cosmology, such as the behavior of the universe at very early times and the formation of structure in the universe. These predictions have been confirmed by observations, providing evidence for the validity of string theory.\n",
      "\n",
      "While string theory is still a highly speculative theory, these reasons have led many physicists to believe that it may be the correct framework for a unified theory of the fundamental forces of nature.\n",
      "\n",
      "Keep in mind that string theory is still an active area of research, and many questions remain unanswered. However, the potential for a unified theory of the fundamental forces of nature makes string theory an exciting and promising area of study.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794be3e",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10521",
   "metadata": {},
   "source": [
    "## Alternate Way : Source Knowledge\n",
    "\n",
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the Deepseek question. We can take the paper abstract from the [Deepseek R1 paper](https://arxiv.org/abs/2501.12948)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9af49e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and \"\n",
    "    \"DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale \"\n",
    "    \"reinforcement learning (RL) without supervised fine-tuning (SFT) as a \"\n",
    "    \"preliminary step, demonstrates remarkable reasoning capabilities. Through \"\n",
    "    \"RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and \"\n",
    "    \"intriguing reasoning behaviors. However, it encounters challenges such as \"\n",
    "    \"poor readability, and language mixing. To address these issues and \"\n",
    "    \"further enhance reasoning performance, we introduce DeepSeek-R1, which \"\n",
    "    \"incorporates multi-stage training and cold-start data before RL. \"\n",
    "    \"DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on \"\n",
    "    \"reasoning tasks. To support the research community, we open-source \"\n",
    "    \"DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, \"\n",
    "    \"32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffaa08",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40297f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf6bb0",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, I think there's been a misunderstanding. The context provided doesn't mention anything about string theory or a unified theory. Instead, it appears to be a discussion about a machine learning model called DeepSeek-R1 and its variants.\n",
      "\n",
      "If you'd like to ask about what makes DeepSeek-R1 special, I can try to answer based on the provided context:\n",
      "\n",
      "DeepSeek-R1 is special because it:\n",
      "\n",
      "1. Incorporates multi-stage training, which is a more sophisticated approach to training machine learning models.\n",
      "2. Utilizes cold-start data before reinforcement learning (RL), which can help improve the model's performance.\n",
      "3. Achieves performance comparable to OpenAI-o1-1217 on reasoning tasks, indicating its ability to reason effectively.\n",
      "4. Is open-sourced, making it available for the research community to use and build upon.\n",
      "5. Has several dense models distilled from it, which can be used for different applications.\n",
      "\n",
      "However, if you'd like to discuss the concept of a unified theory in the context of physics, I can try to provide a separate answer based on the original query about string theory.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b235b09",
   "metadata": {},
   "source": [
    "## How do we get this information in the first place?\n",
    "\n",
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b79b1",
   "metadata": {},
   "source": [
    "This is where Pinecone and vector databases comes in place, as they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b633b59",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/deepseek-r1-paper-chunked\"` dataset. This dataset contains the Deepseek R1 paper pre-processed into RAG-ready chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\shama\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shama\\.cache\\huggingface\\hub\\datasets--jamescalam--deepseek-r1-paper-chunked. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 76/76 [00:00<00:00, 3267.66 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'num_tokens', 'pages', 'source'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/deepseek-r1-paper-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c9178a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2501.12948v1',\n",
       " 'chunk-id': 1,\n",
       " 'chunk': \"uestion: If a > 1, then the sum of the real solutions of √a - √a + x = x is equal to Response: <think> To solve the equation √a – √a + x = x, let's start by squaring both . . . (√a-√a+x)² = x² ⇒ a - √a + x = x². Rearrange to isolate the inner square root term:(a – x²)² = a + x ⇒ a² – 2ax² + (x²)² = a + x ⇒ x⁴ - 2ax² - x + (a² – a) = 0\",\n",
       " 'num_tokens': 145,\n",
       " 'pages': [1],\n",
       " 'source': 'https://arxiv.org/abs/2501.12948'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81d343",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Deepseek R1 ArXiv papers. Each entry in the dataset represents a \"chunk\" of text from the R1 paper.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, even many of the newest LLMs cannot answer questions about Deepseek R1 — at least not without this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a0ede",
   "metadata": {},
   "source": [
    "## Building the Knowledge Base\n",
    "\n",
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our Pinecone client, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize client\n",
    "pc = Pinecone(api_key=pinecone_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78b484",
   "metadata": {},
   "source": [
    "Delete the old one to save the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4aa6f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing indexes: [{\n",
      "    \"name\": \"rag1\",\n",
      "    \"metric\": \"dotproduct\",\n",
      "    \"host\": \"rag1-vzu7ytp.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"cloud\": \"aws\",\n",
      "            \"region\": \"us-east-1\"\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 384,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": null\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "index_name = \"rag1\"\n",
    "print(\"Existing indexes:\", pc.list_indexes())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "545e1533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"rag1\",\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"rag1-vzu7ytp.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec, CloudProvider, AwsRegion, Metric\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    metric=Metric.DOTPRODUCT,\n",
    "    dimension=384,  # ✅ match your embedding model\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=CloudProvider.AWS,\n",
    "        region=AwsRegion.US_EAST_1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df29057",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will HuggingFace's `sentence-transformers/all-MiniLM-L6-v2` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd405ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9229b3d",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5daf97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ea6f5",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) CHANGE-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca587ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Replace this with your real Pinecone key (e.g., pcsk_....)\n",
    "pc = Pinecone(api_key=\"\")\n",
    "\n",
    "# Index name must match exactly what’s created in your Pinecone console\n",
    "index_name = \"rag1\"\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88b28f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e4fa5",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f56bb113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {'': {'vector_count': 76}},\n",
       " 'total_vector_count': 76,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e988a",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86c59e",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to link that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb42eae",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b9846ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embed_model,\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799256a3",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3214e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2501.12948v1-39', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='## 1.2. Summary of Evaluation Results - **Reasoning tasks:** (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge:** On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 4o on this benchmark.'),\n",
       " Document(id='2501.12948v1-56', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='Table summary:\\nTable 5 presents a comparison of DeepSeek-R1 distilled models against other leading models across various reasoning-related benchmarks, including AIME 2024, MATH-500, GPQA Diamond, LiveCode Bench, and CodeForces. The metrics evaluated are pass@1 and cons@64. \\nNotably, the DeepSeek-R1-Distill-Qwen-32B model achieves the highest score in AIME 2024 with 72.6, while the DeepSeek-R1-Distill-Llama-70B excels in MATH-500 with 86.7. In GPQA Diamond, the DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B models lead with scores of 94.3 and 94.5, respectively. \\nOverall, the DeepSeek-R1 models demonstrate competitive performance, particularly the 14B and 32B variants, which consistently rank high across multiple benchmarks. In contrast, models like GPT-4o-0513 and Claude-3.5-Sonnet-1022 show lower performance, especially in AIME 2024 and MATH-500. This analysis highlights the effectiveness of the DeepSeek-R1 distilled models in reasoning tasks, showcasing their potential for further applications in AI-driven reasoning solutions.\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\n| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |\\n| --- | --- | --- | --- | --- | --- |\\n|  | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 |\\n| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 |\\n| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 |\\n| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 |\\n| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |\\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 |\\n| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 |\\n| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 |\\n| DeepSeek-R1-Distill-Qwen-32B | **72.6** | 83.3 | 94.3 | 62.1 | 57.2 |\\n| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 |\\n| DeepSeek-R1-Distill-Llama-70B | 70.0 | **86.7** | **94.5** | **65.2** | **57.5** |'),\n",
       " Document(id='2501.12948v1-4', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='Table summary:\\nThe table presents performance metrics for various AI models across different benchmarks, including AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, measured in accuracy and percentile scores. \\nDeepSeek-R1 shows strong performance with scores of 96.3% and 96.6% at the highest accuracy level (100), while OpenAI-01-1217 achieves 97.3% and 96.4% in the same category. DeepSeek-R1-32B and OpenAI-01-mini exhibit lower scores, particularly in the 80 percentile range, where DeepSeek-V3 performs well with scores of 90.8% and 91.8%. \\nAt lower accuracy levels (60 and 40), DeepSeek models maintain competitive scores, with DeepSeek-R1 recording 72.6% and 39.2% respectively. The table highlights the varying strengths of each model across different tasks, indicating that DeepSeek models generally excel in higher accuracy ranges, while OpenAI models show robust performance in specific benchmarks. This comparative analysis aids in understanding the capabilities and limitations of each AI model in diverse applications.\\n|  | DeepSeek-R1 | OpenAI-01-1217 | DeepSeek-R1-32B | OpenAI-01-mini | DeepSeek-V3 |\\n| --- | --- | --- | --- | --- | --- |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |\\n| 100 |  |  |  |  |  |\\n|  | 96.3 96.6 |  | 97.3 96.4 |  |  |\\n|  |  | 93.4 |  | 94.3 |  |\\n|  |  | 90.6 |  | 90.0 90.2 |  |\\n| 80 | 79.8 79.2 |  |  | 90.8 91.8 | 88.5 |\\n|  |  |  |  |  | 87.4 |\\n|  |  |  |  |  | 85.2 |\\n|  | 72.6 |  |  |  |  |\\n|  |  |  | 75.7 |  |  |\\n|  |  |  | 71.5 |  |  |\\n|  | 63.6 |  | 62.1 |  |  |\\n|  |  |  | 60.0 59.1 |  |  |\\n| 60 |  | 58.7 |  |  |  |\\n| Accuracy / Percentile (%) |  |  |  |  |  |\\n| 40 |  |  |  |  |  |\\n|  | 39.2 |  |  |  |  |\\n| 20 |  |  |  |  |  |\\n| 0 |  |  |  |  |  |\\n|  | AIME 2024 (Pass@1) | Codeforces (Percentile) | GPQA Diamond (Pass@1) | MATH-500 (Pass@1) | MMLU (Pass@1) |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcb169",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to link the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98c418",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25a15337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a526be0",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "769766aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    ## 1.2. Summary of Evaluation Results - **Reasoning tasks:** (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge:** On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 4o on this benchmark.\n",
      "Table summary:\n",
      "Table 5 presents a comparison of DeepSeek-R1 distilled models against other leading models across various reasoning-related benchmarks, including AIME 2024, MATH-500, GPQA Diamond, LiveCode Bench, and CodeForces. The metrics evaluated are pass@1 and cons@64. \n",
      "Notably, the DeepSeek-R1-Distill-Qwen-32B model achieves the highest score in AIME 2024 with 72.6, while the DeepSeek-R1-Distill-Llama-70B excels in MATH-500 with 86.7. In GPQA Diamond, the DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B models lead with scores of 94.3 and 94.5, respectively. \n",
      "Overall, the DeepSeek-R1 models demonstrate competitive performance, particularly the 14B and 32B variants, which consistently rank high across multiple benchmarks. In contrast, models like GPT-4o-0513 and Claude-3.5-Sonnet-1022 show lower performance, especially in AIME 2024 and MATH-500. This analysis highlights the effectiveness of the DeepSeek-R1 distilled models in reasoning tasks, showcasing their potential for further applications in AI-driven reasoning solutions.\n",
      "Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\n",
      "reasoning-related benchmarks.\n",
      "| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "|  | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 |\n",
      "| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 |\n",
      "| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 |\n",
      "| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 |\n",
      "| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |\n",
      "| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 |\n",
      "| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 |\n",
      "| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 |\n",
      "| DeepSeek-R1-Distill-Qwen-32B | **72.6** | 83.3 | 94.3 | 62.1 | 57.2 |\n",
      "| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 |\n",
      "| DeepSeek-R1-Distill-Llama-70B | 70.0 | **86.7** | **94.5** | **65.2** | **57.5** |\n",
      "Table summary:\n",
      "The table presents performance metrics for various AI models across different benchmarks, including AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, measured in accuracy and percentile scores. \n",
      "DeepSeek-R1 shows strong performance with scores of 96.3% and 96.6% at the highest accuracy level (100), while OpenAI-01-1217 achieves 97.3% and 96.4% in the same category. DeepSeek-R1-32B and OpenAI-01-mini exhibit lower scores, particularly in the 80 percentile range, where DeepSeek-V3 performs well with scores of 90.8% and 91.8%. \n",
      "At lower accuracy levels (60 and 40), DeepSeek models maintain competitive scores, with DeepSeek-R1 recording 72.6% and 39.2% respectively. The table highlights the varying strengths of each model across different tasks, indicating that DeepSeek models generally excel in higher accuracy ranges, while OpenAI models show robust performance in specific benchmarks. This comparative analysis aids in understanding the capabilities and limitations of each AI model in diverse applications.\n",
      "|  | DeepSeek-R1 | OpenAI-01-1217 | DeepSeek-R1-32B | OpenAI-01-mini | DeepSeek-V3 |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "| 100 |  |  |  |  |  |\n",
      "|  | 96.3 96.6 |  | 97.3 96.4 |  |  |\n",
      "|  |  | 93.4 |  | 94.3 |  |\n",
      "|  |  | 90.6 |  | 90.0 90.2 |  |\n",
      "| 80 | 79.8 79.2 |  |  | 90.8 91.8 | 88.5 |\n",
      "|  |  |  |  |  | 87.4 |\n",
      "|  |  |  |  |  | 85.2 |\n",
      "|  | 72.6 |  |  |  |  |\n",
      "|  |  |  | 75.7 |  |  |\n",
      "|  |  |  | 71.5 |  |  |\n",
      "|  | 63.6 |  | 62.1 |  |  |\n",
      "|  |  |  | 60.0 59.1 |  |  |\n",
      "| 60 |  | 58.7 |  |  |  |\n",
      "| Accuracy / Percentile (%) |  |  |  |  |  |\n",
      "| 40 |  |  |  |  |  |\n",
      "|  | 39.2 |  |  |  |  |\n",
      "| 20 |  |  |  |  |  |\n",
      "| 0 |  |  |  |  |  |\n",
      "|  | AIME 2024 (Pass@1) | Codeforces (Percentile) | GPQA Diamond (Pass@1) | MATH-500 (Pass@1) | MMLU (Pass@1) |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "\n",
      "    Query: What is so special about Deepseek R1?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59e5e8",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8a399de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1 is a highly advanced artificial intelligence (AI) model that has demonstrated exceptional performance in various reasoning-related tasks. What makes it special is its ability to excel in a wide range of benchmarks, including:\n",
      "\n",
      "1. **High accuracy**: DeepSeek-R1 achieves high accuracy scores in tasks such as AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, with some models reaching scores as high as 96.3% and 96.6%.\n",
      "2. **Competitive performance**: DeepSeek-R1 models consistently rank high across multiple benchmarks, often outperforming other leading models such as OpenAI-01-1217 and Claude-3.5-Sonnet-1022.\n",
      "3. **Robust performance**: DeepSeek-R1 demonstrates robust performance across different tasks and accuracy levels, with some models maintaining competitive scores even at lower accuracy levels (60 and 40).\n",
      "4. **Expert-level performance in coding-related tasks**: DeepSeek-R1 demonstrates expert-level performance in code competition tasks, achieving a 2,029 Elo rating on Codeforces, outperforming 96.3% human participants in the competition.\n",
      "5. **High performance in mathematical reasoning**: DeepSeek-R1 achieves impressive scores in mathematical reasoning tasks, such as MATH-500, with some models reaching scores as high as 97.3%.\n",
      "6. **Competitive performance in educational tasks**: DeepSeek-R1 surpasses other closed-source models in educational tasks, such as MMLU, MMLU-Pro, and GPQA Diamond, with some models achieving scores as high as 90.8%.\n",
      "\n",
      "Overall, DeepSeek-R1 is a highly advanced AI model that has demonstrated exceptional performance in various reasoning-related tasks, making it a strong candidate for real-world applications in AI-driven reasoning solutions.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d917fe",
   "metadata": {},
   "source": [
    "We can continue with another Deepseek R1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cadefe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the contexts provided do not contain any information about DeepSeek-R1-Zero. They only provide information about DeepSeek-R1 and its performance on various benchmarks. Therefore, I cannot provide a comparison between DeepSeek-R1 and DeepSeek-R1-Zero.\n",
      "\n",
      "However, I can suggest that DeepSeek-R1-Zero might be an earlier version or a precursor to DeepSeek-R1, as it is mentioned in the first context as a model that demonstrates \"remarkable reasoning capabilities\" but also has some limitations such as poor readability and language mixing. If you have any more information about DeepSeek-R1-Zero, I may be able to provide a more informed comparison.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"how does deepseek r1 compare to deepseek r1 zero?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef84ba",
   "metadata": {},
   "source": [
    "You can continue asking questions about Deepseek R1, but once you're done you can delete the index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49ac1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67005abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

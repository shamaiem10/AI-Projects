{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c971a5d",
   "metadata": {},
   "source": [
    "# **Build Real-Time Voice-to-Voice Conversational AI Bot üöÄ‚ú®**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71bfe9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd1ff4",
   "metadata": {},
   "source": [
    "## üß† Part 1 ‚Äî Core Concepts (Theory Section)\n",
    "\n",
    "Before building anything, it‚Äôs crucial to understand the building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41770d",
   "metadata": {},
   "source": [
    "### üéôÔ∏è 1. Speech-to-Text (STT)\n",
    "\n",
    "**What it is:**\n",
    "STT converts spoken audio into text. When you speak, your voice is just sound waves. STT uses machine learning models to detect words, convert them into text, and give the computer a readable format.\n",
    "\n",
    "**Real-world examples:**\n",
    "\n",
    "* Siri, Alexa, Google Assistant\n",
    "* Auto-generated captions on YouTube\n",
    "\n",
    "**Popular Tools/Libraries:**\n",
    "\n",
    "* **OpenAI Whisper** (highly accurate, free)\n",
    "* Google Speech-to-Text API\n",
    "* Vosk (offline, open-source)\n",
    "\n",
    "### üîä 2. Text-to-Speech (TTS)\n",
    "\n",
    "**What it is:**\n",
    "TTS converts text back into natural-sounding speech. This allows the bot to ‚Äúspeak‚Äù its response to the user.\n",
    "\n",
    "**Real-world examples:**\n",
    "\n",
    "* Google Maps voice directions\n",
    "* Audible audiobooks generated with AI voices\n",
    "\n",
    "**Popular Tools/Libraries:**\n",
    "\n",
    "* Web Speech API (built into browsers)\n",
    "* gTTS (simple Python library)\n",
    "* ElevenLabs / OpenAI TTS (realistic voices)\n",
    "\n",
    "### üéß 3. Voice Activity Detection (VAD)\n",
    "\n",
    "**What it is:**\n",
    "VAD listens to the microphone and detects when the user is **actually speaking** vs when there is silence or background noise.\n",
    "This is important because you don‚Äôt want to record silence or random noise and waste processing power.\n",
    "\n",
    "**Analogy:**\n",
    "It‚Äôs like a smart recorder that presses ‚Äúrecord‚Äù only when you start talking.\n",
    "\n",
    "**Popular Tools:**\n",
    "\n",
    "* `webrtcvad` (lightweight, fast)\n",
    "* Silero VAD (deep learning based, very accurate)\n",
    "\n",
    "### üß† 4. Large Language Model (LLM)\n",
    "\n",
    "**What it is:**\n",
    "The ‚Äúbrain‚Äù of the chatbot. LLMs are AI models trained on huge amounts of text. They can understand questions, have conversations, and generate human-like text.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* OpenAI GPT (ChatGPT)\n",
    "* LLaMA 2, Falcon, Groq LLM\n",
    "\n",
    "**Role in the Bot:**\n",
    "Takes the **text from STT** as input ‚Üí generates a smart response ‚Üí sends it to TTS.\n",
    "\n",
    "### üåê 5. WebRTC\n",
    "\n",
    "**What it is:**\n",
    "WebRTC (Web Real-Time Communication) is a technology that allows browsers to send and receive **audio, video, and data** directly with very low delay (peer-to-peer).\n",
    "\n",
    "**Why use it:**\n",
    "Perfect for a real-time voice bot because it:\n",
    "\n",
    "* Captures mic input\n",
    "* Streams audio to the backend\n",
    "* Receives audio back instantly\n",
    "* Avoids big delays that make conversations awkward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591b28d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422c0be",
   "metadata": {},
   "source": [
    "## üîÑ Part 2 ‚Äî How the Real-Time Conversational Bot Works (Flow)\n",
    "\n",
    "Once students understand the above concepts, here‚Äôs **how they connect together**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe22e1",
   "metadata": {},
   "source": [
    "### üß© The Pipeline (Step-by-Step)\n",
    "\n",
    "1Ô∏è‚É£ **Capture Audio (WebRTC + VAD)**\n",
    "\n",
    "* Use WebRTC `getUserMedia()` to access the microphone.\n",
    "* Use VAD to start capturing **only when the user is speaking**.\n",
    "* Stream audio chunks to the backend for processing.\n",
    "\n",
    "2Ô∏è‚É£ **Convert Speech to Text (STT)**\n",
    "\n",
    "* Backend receives audio chunks and passes them to STT engine (e.g., Whisper).\n",
    "* Get the transcribed text output.\n",
    "\n",
    "3Ô∏è‚É£ **Generate Response (LLM)**\n",
    "\n",
    "* Send transcribed text to an LLM (Groq / OpenAI / Hugging Face).\n",
    "* Receive a smart, human-like response in text form.\n",
    "\n",
    "4Ô∏è‚É£ **Convert Text to Speech (TTS)**\n",
    "\n",
    "* Pass the LLM‚Äôs response text to a TTS engine (gTTS / Web Speech API / ElevenLabs).\n",
    "* Generate speech audio file or audio stream.\n",
    "\n",
    "5Ô∏è‚É£ **Play Response (WebRTC)**\n",
    "\n",
    "* Stream the generated speech back to the browser.\n",
    "* Play it instantly, so the user hears the bot‚Äôs reply.\n",
    "\n",
    "### üñºÔ∏è Visual Flow (Simple Diagram)\n",
    "\n",
    "```\n",
    "üé§ User Speaks \n",
    "   ‚Üì (WebRTC + VAD)\n",
    "üéôÔ∏è Audio Stream ‚Üí [STT Engine] ‚Üí üìù Text\n",
    "   ‚Üì\n",
    "üß† [LLM/NLP Model] ‚Üí üí¨ Response Text\n",
    "   ‚Üì\n",
    "üîä [TTS Engine] ‚Üí üéµ Speech\n",
    "   ‚Üì (WebRTC)\n",
    "üó£Ô∏è Bot Speaks Back\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58108fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da5ec5",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 3 ‚Äî Your Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1db118",
   "metadata": {},
   "source": [
    "**Implement a basic pipeline** (does not need to be perfect):\n",
    "\n",
    "   * Capture audio (WebRTC)\n",
    "   * Convert speech to text (STT)\n",
    "   * Send to LLM (even a simple rule-based chatbot is fine for MVP)\n",
    "   * Convert text to speech (TTS)\n",
    "   * Play the response\n",
    "\n",
    "## üí° Bonus Points\n",
    "\n",
    "* Stream partial transcripts while user is speaking. (e.g: Humans can interrupt bot)\n",
    "* Add memory so the bot remembers the last few turns.\n",
    "* Deploy your bot on a simple web app using `Flask + WebRTC` or `Streamlit`.\n",
    "\n",
    "Here is a very basic and east conversational AI bot: https://github.com/momina02/Conversational-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9c6f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca38d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0e2a3",
   "metadata": {},
   "source": [
    "# üé§ AURI ‚Äì AI Voice Chatbot: Tech Stack Deep Dive\n",
    "\n",
    "**AURI** is a sophisticated **<span style=\"color:#FF69B4\">real-time AI voice chatbot</span>** that combines multiple cutting-edge technologies to provide an interactive and human-like conversational experience. The project leverages a carefully curated tech stack spanning **backend, frontend, AI services**, and **auxiliary tools** to deliver a seamless real-time voice-to-voice interface. Below is a detailed explanation of the technologies used and their roles in the project.\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è Backend\n",
    "\n",
    "1. **<span style=\"color:#FFD700\">Flask</span>**  \n",
    "   - Flask is a lightweight web framework used to handle HTTP routing, render templates, and serve APIs. In this project, Flask manages the frontend-backend interaction, receives audio files from users, communicates with AI services, and sends processed responses back to the frontend.\n",
    "\n",
    "2. **<span style=\"color:#FF69B4\">AssemblyAI (Speech-to-Text)</span>**  \n",
    "   - AssemblyAI provides reliable transcription services. User voice input is recorded in the browser and sent to Flask, which uploads the audio to AssemblyAI. The transcription API returns the text representation of the user‚Äôs speech, which is essential for feeding the LLM (Groq) with accurate inputs.\n",
    "\n",
    "3. **<span style=\"color:#FFD700\">Groq LLM (Large Language Model)</span>**  \n",
    "   - The Groq Large Language Model generates intelligent and context-aware responses. The last 5 user-bot conversation pairs are sent along with the current user input, enabling the bot to maintain memory and respond coherently. This ensures a conversational flow rather than isolated question-answer pairs.\n",
    "\n",
    "4. **<span style=\"color:#FF69B4\">gTTS (Google Text-to-Speech)</span>**  \n",
    "   - Converts the bot‚Äôs text responses into audio files, allowing the user to **hear** the AI response. gTTS provides natural-sounding TTS without requiring complex AI voice model deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Frontend\n",
    "\n",
    "1. **<span style=\"color:#FFD700\">HTML5 & CSS3</span>**  \n",
    "   - Provides the structural skeleton and styling for the chatbot UI. The interface uses a **pink, light yellow, and black theme** with **horizontal layout** to give a modern chat-app feel.\n",
    "\n",
    "2. **<span style=\"color:#FF69B4\">JavaScript</span>**  \n",
    "   - Handles **real-time audio recording** using the MediaRecorder API, sends audio to the backend via fetch API, updates chat bubbles dynamically, and plays back bot responses.  \n",
    "\n",
    "3. **<span style=\"color:#FFD700\">Bootstrap & Bootstrap Icons</span>**  \n",
    "   - Provides a clean, responsive UI framework and icons for user controls such as mic buttons, stop recording buttons, and aesthetic enhancements.  \n",
    "   - Ensures a polished, professional look across devices without heavy custom CSS.\n",
    "\n",
    "4. **<span style=\"color:#FF69B4\">Dynamic Chat Bubbles & Memory Display</span>**  \n",
    "   - Alternating **left (bot)** and **right (user)** chat bubbles replicate a WhatsApp-style interface.  \n",
    "   - Maintains a visible **conversation memory** of the last 5 interactions for context awareness.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Workflow Overview\n",
    "\n",
    "1. **User Interaction**: The user records their voice in the browser.  \n",
    "2. **Audio Upload**: JavaScript sends the audio file to the Flask backend.  \n",
    "3. **Transcription**: Flask uploads the audio to AssemblyAI and polls until transcription is complete.  \n",
    "4. **Context Management**: The last 5 interactions plus the current user text are sent to Groq LLM.  \n",
    "5. **AI Response**: Groq generates a coherent, context-aware reply.  \n",
    "6. **Text-to-Speech**: gTTS converts the AI response into an audio file.  \n",
    "7. **Frontend Update**: Flask returns text + audio URL, and JavaScript dynamically updates chat bubbles and plays audio.  \n",
    "\n",
    "---\n",
    "\n",
    "## üì∏ Screenshot\n",
    "\n",
    "![AURI Chatbot Screenshot](chatbot_ss.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîó GitHub Repository\n",
    "\n",
    "You can explore the complete project and source code here:  \n",
    "[https://github.com/shamaiem10/Auri](https://github.com/shamaiem10/Auri)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
